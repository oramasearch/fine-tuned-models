model:
  name: "Qwen/Qwen2.5-7B"
  sequence_length: 512
  batch_size: 16
  gradient_accumulation_steps: 2

lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules:
    - "self_attn.q_proj"
    - "self_attn.k_proj"
    - "self_attn.v_proj"
    - "self_attn.o_proj"
    - "mlp.gate_proj"
    - "mlp.up_proj"
    - "mlp.down_proj"
  modules_to_save:
    - "embed_tokens"
    - "lm_head"

training:
  learning_rate: 1e-4
  num_epochs: 3
  warmup_ratio: 0.1 
  eval_steps: 100
  save_steps: 100
  max_steps: 1000
  output_dir: "./query-translator-mini"

data:
  path: "./synthetic_data.jsonl"
  test_size: 0.1
  num_proc: 4